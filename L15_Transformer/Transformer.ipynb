{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.8.10 64-bit"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"cells":[{"cell_type":"markdown","source":["# Thực hành Transformers"],"metadata":{"id":"8bYq78TsaoN2"}},{"cell_type":"markdown","source":["Trong bài này, ta sẽ thực hành cài đặt Transformer"],"metadata":{"id":"hGpgYCrAaoOE"}},{"cell_type":"markdown","source":["### 1. Cài đặt và import thư viện"],"metadata":{"id":"aZV-ISjJaoOG"}},{"cell_type":"code","execution_count":null,"source":["!which python3"],"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/bin/python3\n"]}],"metadata":{"id":"3WdMvH4x9swj","outputId":"c2b1e41a-b6e0-4563-f7e5-2e5fc978107b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644744678282,"user_tz":-420,"elapsed":45,"user":{"displayName":"Minh Sơn Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2j1sDLJ9nFTbjMwh1pIeQuSUiyuJSlxHgxuMc=s64","userId":"03159414786129788503"}}}},{"cell_type":"code","execution_count":null,"source":["!pip3 install spacy dill\n","!pip3 install torchtext\n","!pip3 install pandas"],"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (0.3.4)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.10.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.11.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.62.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.19.5)\n","Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.10.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchtext) (3.10.0.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"]}],"metadata":{"id":"I4ObXVBqjGby","outputId":"6a8a2d3d-c453-4621-95be-a181c2f65022","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644744874092,"user_tz":-420,"elapsed":11610,"user":{"displayName":"Minh Sơn Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2j1sDLJ9nFTbjMwh1pIeQuSUiyuJSlxHgxuMc=s64","userId":"03159414786129788503"}}}},{"cell_type":"code","execution_count":null,"source":["!python3 -m spacy download en && python3 -m spacy download fr"],"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en_core_web_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n","\u001b[K     |████████████████████████████████| 12.0 MB 12.8 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.10.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","\u001b[38;5;2m✔ Linking successful\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n","/usr/local/lib/python3.7/dist-packages/spacy/data/en\n","You can now load the model via spacy.load('en')\n","Collecting fr_core_news_sm==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz (14.7 MB)\n","\u001b[K     |████████████████████████████████| 14.7 MB 14.5 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.6)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.9.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.62.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (57.4.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.19.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.6)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.10.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n","Building wheels for collected packages: fr-core-news-sm\n","  Building wheel for fr-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fr-core-news-sm: filename=fr_core_news_sm-2.2.5-py3-none-any.whl size=14727025 sha256=8075bea2ea5c8855781dd058db750c6ab978844b634781fc7a0bf2691d920657\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-neqq1a7o/wheels/c9/a6/ea/0778337c34660027ee67ef3a91fb9d3600b76777a912ea1c24\n","Successfully built fr-core-news-sm\n","Installing collected packages: fr-core-news-sm\n","Successfully installed fr-core-news-sm-2.2.5\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('fr_core_news_sm')\n","\u001b[38;5;2m✔ Linking successful\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/fr_core_news_sm -->\n","/usr/local/lib/python3.7/dist-packages/spacy/data/fr\n","You can now load the model via spacy.load('fr')\n"]}],"metadata":{"id":"US4j_5D69swl","outputId":"115de625-b7e2-489c-9826-53aa62c34f4d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644758694350,"user_tz":-420,"elapsed":16177,"user":{"displayName":"Minh Sơn Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2j1sDLJ9nFTbjMwh1pIeQuSUiyuJSlxHgxuMc=s64","userId":"03159414786129788503"}}}},{"cell_type":"code","execution_count":null,"source":["import torch.nn as nn\n","import torch\n","import torchtext\n","import copy\n","import math\n","import torch.nn.functional as F\n"],"outputs":[],"metadata":{"id":"OaNedrhUjGb0"}},{"cell_type":"code","execution_count":null,"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"outputs":[],"metadata":{"id":"I6462QxLjGb0"}},{"cell_type":"markdown","source":["### 2. Cài đặt từng module của Transformer"],"metadata":{"id":"0xf-aVKQaoOQ"}},{"cell_type":"code","execution_count":null,"source":["class Embedder(nn.Module):\n","    def __init__(self, vocab_size, dim):\n","        super().__init__()\n","        self.embed = nn.Embedding(vocab_size, dim)\n","    \n","    def forward(self, x):\n","        return self.embed(x)"],"outputs":[],"metadata":{"id":"DosoYt1YjGb0"}},{"cell_type":"markdown","source":["**Position Embedding Class**:"],"metadata":{"id":"pBaPrBG0ubI-"}},{"cell_type":"code","execution_count":null,"source":["# Positional encoding\n","class PositionalEncoder(nn.Module):\n","    def __init__(self, dim, max_seq_len=300):\n","        super().__init__()\n","        self.dim = dim\n","        \n","        # create a constant 'pe' matrix with values dependant on \n","        # pos and i\n","        pe = torch.zeros(max_seq_len, dim)\n","        for pos in range(max_seq_len):\n","            for i in range(0, dim, 2):\n","                pe[pos, i] = math.sin(pos/ (10000 ** ((2*i)/dim)))\n","                pe[pos, i+1] = math.cos(pos / (10000 ** ((2* (i+1))/dim)))\n","        \n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","        \n","    def forward(self, x):\n","        # make embeddings relatively larger\n","        x = x *math.sqrt(self.dim)\n","        # add constant to embedding\n","        seq_len = x.size(1)\n","        x = x + Variable(self.pe[:, :seq_len], requires_grad=False).to(device)\n","        return x"],"outputs":[],"metadata":{"id":"MJs0_6GwjGb1"}},{"cell_type":"markdown","source":["**Multi Head Attention**: We first start with implementing attention function\n","\n","Attention of $q$"],"metadata":{"id":"F25tcm9tu1ce"}},{"cell_type":"code","execution_count":null,"source":["def attention(q, k, v, d_k, mask=None, dropout=None):\n","    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n","    if mask is not None:\n","        mask = mask.unsqueeze(1)\n","        scores = scores.masked_fill(mask == 0, -1e9)\n","    \n","    scores = F.softmax(scores, dim=-1)\n","    \n","    if dropout is not None:\n","        scores = dropout(scores)\n","        \n","    output = torch.matmul(scores, v)\n","    return output\n","\n","def attention(q, k, v d_k, mask= None, dropout=None):\n","    scores = torch.matmul(q, k.transpose)"],"outputs":[],"metadata":{"id":"wJEKoan4jGb2"}},{"cell_type":"code","execution_count":null,"source":["# Multi-headed attention\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, heads, dim, dropout=0.1):\n","        super().__init__()\n","        self.dim = dim\n","        self.dim_head = dim//heads\n","        self.h = heads\n","        self.q_linear = nn.Linear(dim, dim)\n","        self.k_linear = nn.Linear(dim, dim)\n","        self.v_linear = nn.Linear(dim, dim)\n","        self.dropout = nn.Dropout(dropout)\n","        self.out = nn.Linear(dim, dim)\n","    \n","    def forward(self, q, k, v, mask=None):\n","        bs = q.size(0)\n","        # perform linear operation and split into h heads\n","        k = self.k_linear(k).view(bs, -1, self.h, self.dim_head)\n","        q = self.q_linear(q).view(bs, -1, self.h, self.dim_head)\n","        v = self.v_linear(v).view(bs, -1, self.h, self.dim_head)\n","        # transpose to get dimensions bs * h * sl * dim\n","        k = k.transpose(1, 2)\n","        q = q.transpose(1, 2)\n","        v = v.transpose(1, 2)\n","        # calculate attention using the function we will define next\n","        scores = attention(q, k, v, self.dim, mask, self.dropout)\n","        # concatenate heads and put through final linear layer\n","        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.dim)\n","        output = self.out(concat)\n","        return output"],"outputs":[],"metadata":{"id":"4vvVcNXBjGb1"}},{"cell_type":"code","execution_count":null,"source":["class FeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n","        super().__init__() \n","        # We set d_ff as a default to 2048\n","        self.linear_1 = nn.Linear(d_model, d_ff)\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear_2 = nn.Linear(d_ff, d_model)\n","    def forward(self, x):\n","        x = self.dropout(F.relu(self.linear_1(x)))\n","        x = self.linear_2(x)\n","        return x"],"outputs":[],"metadata":{"id":"2Cy0Xt9QjGb2"}},{"cell_type":"code","execution_count":null,"source":["class Norm(nn.Module):\n","    def __init__(self, d_model, eps = 1e-6):\n","        super().__init__()\n","    \n","        self.size = d_model\n","        # create two learnable parameters to calibrate normalisation\n","        self.alpha = nn.Parameter(torch.ones(self.size))\n","        self.bias = nn.Parameter(torch.zeros(self.size))\n","        self.eps = eps\n","    def forward(self, x):\n","        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n","        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n","        return norm"],"outputs":[],"metadata":{"id":"v7UTrblYjGb2"}},{"cell_type":"code","execution_count":null,"source":["# build an encoder layer with one multi-head attention layer and one \n","# feed-forward layer\n","class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, heads, dropout = 0.1):\n","        super().__init__()\n","        self.norm_1 = Norm(d_model)\n","        self.norm_2 = Norm(d_model)\n","        self.attn = MultiHeadAttention(heads, d_model)\n","        self.ff = FeedForward(d_model)\n","        self.dropout_1 = nn.Dropout(dropout)\n","        self.dropout_2 = nn.Dropout(dropout)\n","        \n","    def forward(self, x, mask):\n","        x2 = self.norm_1(x)\n","        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n","        x2 = self.norm_2(x)\n","        x = x + self.dropout_2(self.ff(x2))\n","        return x\n","    "],"outputs":[],"metadata":{"id":"1uYXmKKsjGb2"}},{"cell_type":"code","execution_count":null,"source":["# build a decoder layer with two multi-head attention layers and\n","# one feed-forward layer\n","class DecoderLayer(nn.Module):\n","    def __init__(self, d_model, heads, dropout=0.1):\n","        super().__init__()\n","        self.norm_1 = Norm(d_model)\n","        self.norm_2 = Norm(d_model)\n","        self.norm_3 = Norm(d_model)\n","        \n","        self.dropout_1 = nn.Dropout(dropout)\n","        self.dropout_2 = nn.Dropout(dropout)\n","        self.dropout_3 = nn.Dropout(dropout)\n","        \n","        self.attn_1 = MultiHeadAttention(heads, d_model)\n","        self.attn_2 = MultiHeadAttention(heads, d_model)\n","        self.ff = FeedForward(d_model).cuda()\n","    \n","    def forward(self, x, e_outputs, src_mask, trg_mask):\n","        x2 = self.norm_1(x)\n","        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n","        x2 = self.norm_2(x)\n","        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,\n","        src_mask))\n","        x2 = self.norm_3(x)\n","        x = x + self.dropout_3(self.ff(x2))\n","        return x\n","\n","def get_clones(module, N):\n","    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"],"outputs":[],"metadata":{"id":"8AWL51G_jGb3"}},{"cell_type":"code","execution_count":null,"source":["class Encoder(nn.Module):\n","    def __init__(self, vocab_size, d_model, N, heads):\n","        super().__init__()\n","        self.N = N\n","        self.embed = Embedder(vocab_size, d_model)\n","        self.pe = PositionalEncoder(d_model)\n","        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n","        self.norm = Norm(d_model)\n","        \n","    def forward(self, src, mask):\n","        x = self.embed(src)\n","        x = self.pe(x)\n","        for i in range(self.N):\n","            x = self.layers[i](x, mask)\n","        return self.norm(x)\n","    \n","class Decoder(nn.Module):\n","    def __init__(self, vocab_size, d_model, N, heads):\n","        super().__init__()\n","        self.N = N\n","        self.embed = Embedder(vocab_size, d_model)\n","        self.pe = PositionalEncoder(d_model)\n","        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n","        self.norm = Norm(d_model)\n","    def forward(self, trg, e_outputs, src_mask, trg_mask):\n","        x = self.embed(trg)\n","        x = self.pe(x)\n","        for i in range(self.N):\n","            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n","        return self.norm(x)"],"outputs":[],"metadata":{"id":"mQAOcVwqjGb3"}},{"cell_type":"code","execution_count":null,"source":["class Transformer(nn.Module):\n","    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n","        super().__init__()\n","        self.encoder = Encoder(src_vocab, d_model, N, heads)\n","        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n","        self.out = nn.Linear(d_model, trg_vocab)\n","    def forward(self, src, trg, src_mask, trg_mask):\n","        e_outputs = self.encoder(src, src_mask)\n","        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n","        output = self.out(d_output)\n","        return output# we don't perform softmax on the output as this will be handled \n","# automatically by our loss function"],"outputs":[],"metadata":{"id":"TtVVMjazjGb3"}},{"cell_type":"markdown","source":["### 3. Chuẩn bị và tiền xử lý dữ liệu"],"metadata":{"id":"wSU0Ubc8aoOi"}},{"cell_type":"code","execution_count":null,"source":["import spacy\n","import re\n","\n","# Tokenize\n","\n","class tokenize(object):\n","    \n","    def __init__(self, lang):\n","        self.nlp = spacy.load(lang)\n","            \n","    def tokenizer(self, sentence):\n","        sentence = re.sub(\n","        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n","        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n","        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n","        sentence = re.sub(r\"\\,+\", \",\", sentence)\n","        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n","        sentence = sentence.lower()\n","        return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]"],"outputs":[],"metadata":{"id":"ZY3841jbjGb4"}},{"cell_type":"code","execution_count":null,"source":["# Creating batch\n","from torchtext.legacy import data\n","import numpy as np\n","from torch.autograd import Variable\n","\n","\n","def nopeak_mask(size, opt):\n","    np_mask = np.triu(np.ones((1, size, size)),\n","    k=1).astype('uint8')\n","    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n","    np_mask = np_mask.to(device)\n","    return np_mask\n","\n","def create_masks(src, trg, opt):\n","    \n","    src_mask = (src != opt.src_pad).unsqueeze(-2)\n","\n","    if trg is not None:\n","        trg.to(device)\n","        trg_mask = (trg != opt.trg_pad).unsqueeze(-2).to(device)\n","        size = trg.size(1) # get seq_len for matrix\n","        np_mask = nopeak_mask(size, opt)\n","        trg_mask = trg_mask & np_mask\n","        \n","    else:\n","        trg_mask = None\n","    return src_mask, trg_mask\n","\n","# patch on Torchtext's batching process that makes it more efficient\n","# from http://nlp.seas.harvard.edu/2018/04/03/attention.html#position-wise-feed-forward-networks\n","\n","class MyIterator(data.Iterator):\n","    def create_batches(self):\n","        if self.train:\n","            def pool(d, random_shuffler):\n","                for p in data.batch(d, self.batch_size * 100):\n","                    p_batch = data.batch(\n","                        sorted(p, key=self.sort_key),\n","                        self.batch_size, self.batch_size_fn)\n","                    for b in random_shuffler(list(p_batch)):\n","                        yield b\n","            self.batches = pool(self.data(), self.random_shuffler)\n","            \n","        else:\n","            self.batches = []\n","            for b in data.batch(self.data(), self.batch_size,\n","                                          self.batch_size_fn):\n","                self.batches.append(sorted(b, key=self.sort_key))\n","\n","global max_src_in_batch, max_tgt_in_batch\n","\n","def batch_size_fn(new, count, sofar):\n","    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n","    global max_src_in_batch, max_tgt_in_batch\n","    if count == 1:\n","        max_src_in_batch = 0\n","        max_tgt_in_batch = 0\n","    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n","    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n","    src_elements = count * max_src_in_batch\n","    tgt_elements = count * max_tgt_in_batch\n","    return max(src_elements, tgt_elements)"],"outputs":[],"metadata":{"id":"xLDB_d_mjGb4"}},{"cell_type":"code","execution_count":null,"source":["import pandas as pd\n","import torchtext\n","from torchtext.legacy import data\n","import os\n","import dill as pickle\n","\n","def read_data(opt):\n","    if opt.src_data is not None:\n","        try:\n","            opt.src_data = open(opt.src_data).read().strip().split('\\n')\n","        except:\n","            print(\"error: '\" + opt.src_data + \"' file not found\")\n","            quit()\n","    \n","    if opt.trg_data is not None:\n","        try:\n","            opt.trg_data = open(opt.trg_data).read().strip().split('\\n')\n","        except:\n","            print(\"error: '\" + opt.trg_data + \"' file not found\")\n","            quit()\n","\n","def create_fields(opt):\n","    spacy_langs = ['en', 'fr', 'de', 'es', 'pt', 'it', 'nl']\n","    src_lang = opt.src_lang[0:2]\n","    trg_lang = opt.trg_lang[0:2]\n","    if src_lang not in spacy_langs:\n","        print('invalid src language: ' + opt.src_lang + 'supported languages : ' + spacy_langs)  \n","    if trg_lang not in spacy_langs:\n","        print('invalid trg language: ' + opt.trg_lang + 'supported languages : ' + spacy_langs)\n","    \n","    print(\"loading spacy tokenizers...\")\n","    \n","    t_src = tokenize(opt.src_lang)\n","    t_trg = tokenize(opt.trg_lang)\n","    TRG = data.Field(lower=True, tokenize=t_trg.tokenizer, init_token='<sos>', eos_token='<eos>')\n","    SRC = data.Field(lower=True, tokenize=t_src.tokenizer)\n","\n","    return(SRC, TRG)\n","\n","def create_dataset(opt, SRC, TRG):\n","\n","    print(\"creating dataset and iterator... \")\n","\n","    raw_data = {'src' : [line for line in opt.src_data], 'trg': [line for line in opt.trg_data]}\n","    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n","    \n","    mask = (df['src'].str.count(' ') < opt.max_strlen) & (df['trg'].str.count(' ') < opt.max_strlen)\n","    df = df.loc[mask]\n","\n","    df.to_csv(\"translate_transformer_temp.csv\", index=False)\n","    \n","    data_fields = [('src', SRC), ('trg', TRG)]\n","    train = data.TabularDataset('./translate_transformer_temp.csv', format='csv', fields=data_fields)\n","\n","    train_iter = MyIterator(train, batch_size=opt.batchsize, device=device,\n","                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n","                        batch_size_fn=batch_size_fn, train=True, shuffle=True)\n","    \n","    os.remove('translate_transformer_temp.csv')\n","    SRC.build_vocab(train)\n","    TRG.build_vocab(train)\n","    opt.src_pad = SRC.vocab.stoi['<pad>']\n","    opt.trg_pad = TRG.vocab.stoi['<pad>']\n","\n","    opt.train_len = get_len(train_iter)\n","\n","    return train_iter\n","\n","def get_len(train):\n","\n","    for i, b in enumerate(train):\n","        pass\n","    \n","    return i"],"outputs":[],"metadata":{"id":"T0jx8nh0jGb5"}},{"cell_type":"markdown","source":["### 4. Cài đặt giải thuật tối ưu và huấn luyện mô hình"],"metadata":{"id":"Irad1fZjaoOl"}},{"cell_type":"code","execution_count":null,"source":["# Optimizer\n","class CosineWithRestarts(torch.optim.lr_scheduler._LRScheduler):\n","    \"\"\"\n","    Cosine annealing with restarts.\n","    Parameters\n","    ----------\n","    optimizer : torch.optim.Optimizer\n","    T_max : int\n","        The maximum number of iterations within the first cycle.\n","    eta_min : float, optional (default: 0)\n","        The minimum learning rate.\n","    last_epoch : int, optional (default: -1)\n","        The index of the last epoch.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 optimizer: torch.optim.Optimizer,\n","                 T_max: int,\n","                 eta_min: float = 0.,\n","                 last_epoch: int = -1,\n","                 factor: float = 1.) -> None:\n","        # pylint: disable=invalid-name\n","        self.T_max = T_max\n","        self.eta_min = eta_min\n","        self.factor = factor\n","        self._last_restart: int = 0\n","        self._cycle_counter: int = 0\n","        self._cycle_factor: float = 1.\n","        self._updated_cycle_len: int = T_max\n","        self._initialized: bool = False\n","        super(CosineWithRestarts, self).__init__(optimizer, last_epoch)\n","\n","    def get_lr(self):\n","        \"\"\"Get updated learning rate.\"\"\"\n","        # HACK: We need to check if this is the first time get_lr() was called, since\n","        # we want to start with step = 0, but _LRScheduler calls get_lr with\n","        # last_epoch + 1 when initialized.\n","        if not self._initialized:\n","            self._initialized = True\n","            return self.base_lrs\n","\n","        step = self.last_epoch + 1\n","        self._cycle_counter = step - self._last_restart\n","\n","        lrs = [\n","            (\n","                self.eta_min + ((lr - self.eta_min) / 2) *\n","                (\n","                    np.cos(\n","                        np.pi *\n","                        ((self._cycle_counter) % self._updated_cycle_len) /\n","                        self._updated_cycle_len\n","                    ) + 1\n","                )\n","            ) for lr in self.base_lrs\n","        ]\n","\n","        if self._cycle_counter % self._updated_cycle_len == 0:\n","            # Adjust the cycle length.\n","            self._cycle_factor *= self.factor\n","            self._cycle_counter = 0\n","            self._updated_cycle_len = int(self._cycle_factor * self.T_max)\n","            self._last_restart = step\n","\n","        return lrs\n"],"outputs":[],"metadata":{"id":"hvXmikJB9swr"}},{"cell_type":"code","execution_count":null,"source":["!mkdir data\n","!wget https://raw.githubusercontent.com/SamLynnEvans/Transformer/master/data/english.txt\n","!mv english.txt data\n","!wget https://raw.githubusercontent.com/SamLynnEvans/Transformer/master/data/french.txt data/french.txt\n","!mv french.txt data"],"outputs":[],"metadata":{"id":"TRTtm5kO9swr"}},{"cell_type":"code","execution_count":null,"source":["\n","def get_model(opt, src_vocab, trg_vocab):\n","    \n","    assert opt.d_model % opt.heads == 0\n","    assert opt.dropout < 1\n","\n","    model = Transformer(src_vocab, trg_vocab, opt.d_model, opt.n_layers, opt.heads)\n","       \n","    if opt.load_weights is not None:\n","        print(\"loading pretrained weights...\")\n","        model.load_state_dict(torch.load(f'{opt.load_weights}/model_weights'))\n","    else:\n","        for p in model.parameters():\n","            if p.dim() > 1:\n","                nn.init.xavier_uniform_(p) \n","    \n","    if opt.device == 0:\n","        model = model.cuda()\n","    \n","    return model\n","\n","\n","\n"],"outputs":[],"metadata":{"id":"LSOX2OEW9swr"}},{"cell_type":"code","execution_count":null,"source":["import time\n","\n","def train_model(model, opt):\n","    \n","    print(\"training model...\")\n","    model.train()\n","    start = time.time()\n","    if opt.checkpoint > 0:\n","        cptime = time.time()\n","\n","\n","                 \n","    for epoch in range(opt.epochs):\n","\n","        total_loss = 0\n","        print(\"   %dm: epoch %d [%s]  %d%%  loss = %s\" %\\\n","            ((time.time() - start)//60, epoch + 1, \"\".join(' '*20), 0, '...'), end='\\r')\n","        \n","        if opt.checkpoint > 0:\n","            torch.save(model.state_dict(), 'weights/model_weights')\n","                    \n","        for i, batch in enumerate(opt.train): \n","\n","\n","\n","            src = batch.src.transpose(0,1).to(device)\n","            trg = batch.trg.transpose(0,1).to(device)\n","            trg_input = trg[:, :-1].to(device)\n","            src_mask, trg_mask = create_masks(src, trg_input, opt)\n","            preds = model(src, trg_input, src_mask, trg_mask)\n","            ys = trg[:, 1:].contiguous().view(-1)\n","            opt.optimizer.zero_grad()\n","            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=opt.trg_pad)\n","            loss.backward()\n","            opt.optimizer.step()\n","          \n","            total_loss += loss.item()\n","            \n","            if (i + 1) % opt.printevery == 0:\n","                p = int(100 * (i + 1) / opt.train_len)\n","                avg_loss = total_loss/opt.printevery\n","                print(\"   %dm: epoch %d [%s%s]  %d%%  loss = %.3f\" %\\\n","                    ((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_loss))\n","                total_loss = 0\n","            \n","            if opt.checkpoint > 0 and ((time.time()-cptime)//60) // opt.checkpoint >= 1:\n","                torch.save(model.state_dict(), 'weights/model_weights')\n","                cptime = time.time()\n","   \n","   \n","        print(\"%dm: epoch %d [%s%s]  %d%%  loss = %.3f\\nepoch %d complete, loss = %.03f\" %\\\n","        ((time.time() - start)//60, epoch + 1, \"\".join('#'*(100//5)), \"\".join(' '*(20-(100//5))), 100, avg_loss, epoch + 1, avg_loss))\n","\n","class Opt(object):\n","    pass\n","        \n","def main():\n","    opt = Opt()\n","    opt.src_data = \"data/english.txt\"\n","    opt.trg_data = \"data/french.txt\"\n","    opt.src_lang = \"en_core_web_sm\"\n","    opt.trg_lang = 'fr_core_news_sm'\n","    opt.epochs = 2\n","    opt.d_model=512\n","    opt.n_layers=6\n","    opt.heads=8\n","    opt.dropout=0.1\n","    opt.batchsize=1500\n","    opt.printevery=100\n","    opt.lr=0.0001\n","    opt.max_strlen=80\n","    opt.checkpoint = 0\n","    opt.no_cuda = False\n","    opt.load_weights = None\n","    \n","    opt.device = 0\n","    if opt.device == 0:\n","        assert torch.cuda.is_available()\n","    \n","    read_data(opt)\n","    SRC, TRG = create_fields(opt)\n","    opt.train = create_dataset(opt, SRC, TRG)\n","    model = get_model(opt, len(SRC.vocab), len(TRG.vocab)).to(device)\n","\n","    opt.optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr, betas=(0.9, 0.98), eps=1e-9)\n","\n","    if opt.checkpoint > 0:\n","        print(\"model weights will be saved every %d minutes and at end of epoch to directory weights/\"%(opt.checkpoint))\n","    \n","    train_model(model, opt)\n","\n","\n","    # for asking about further training use while true loop, and return\n","if __name__ == \"__main__\":\n","    main()"],"outputs":[],"metadata":{"id":"0Q2qOSP7jGb5"}}]}